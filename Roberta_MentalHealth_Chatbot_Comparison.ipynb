{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Dependencies**"
      ],
      "metadata": {
        "id": "G1ejnSNtXWeF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEoWRQ3j9k4T"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "id": "-yp4QRC0gOau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RoBERTa model**"
      ],
      "metadata": {
        "id": "CPKhGVANEU6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the dataset and training the model**"
      ],
      "metadata": {
        "id": "XEQTcKD-XgHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "dataset = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\")\n",
        "texts = dataset['train']['text']\n",
        "\n",
        "# Step 2: Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Step 3: Tokenize the conversations\n",
        "max_seq_length = 512  # Maximum sequence length supported by RoBERTa\n",
        "tokenized_conversations = []\n",
        "for text in texts:\n",
        "    tokenized_text = tokenizer.encode(text, max_length=max_seq_length, truncation=True)\n",
        "    tokenized_conversations.append(torch.tensor(tokenized_text))\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, conversations, max_seq_length):\n",
        "        self.conversations = conversations\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        # Pad or truncate the conversation to the maximum sequence length\n",
        "        conversation = conversation[:self.max_seq_length]\n",
        "        # Pad the conversation if it's shorter than max_seq_length\n",
        "        conversation = torch.nn.functional.pad(conversation, (0, self.max_seq_length - len(conversation)))\n",
        "        # Return both the input_ids and target_ids\n",
        "        return conversation, conversation\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ChatDataset(tokenized_conversations, max_seq_length)\n",
        "\n",
        "# Step 5: Set up the model\n",
        "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", is_decoder=True)\n",
        "\n",
        "# Step 6: Set up the training parameters\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 100\n",
        "\n",
        "# Step 7: Set up the optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset) * num_epochs)\n",
        "\n",
        "# Fine-tune the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for data in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataset)}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot_models/roberta.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX23mgfR607_",
        "outputId": "f4cab984-d731-48de-f494-5d6c01ccfc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.5990524780611659\n",
            "Epoch 2/100, Loss: 0.3496653149987376\n",
            "Epoch 3/100, Loss: 0.2895550540713377\n",
            "Epoch 4/100, Loss: 0.2541965055604314\n",
            "Epoch 5/100, Loss: 0.2262523018343504\n",
            "Epoch 6/100, Loss: 0.20606824183879896\n",
            "Epoch 7/100, Loss: 0.22305577560219653\n",
            "Epoch 8/100, Loss: 0.19355169111906095\n",
            "Epoch 9/100, Loss: 0.15657362636438635\n",
            "Epoch 10/100, Loss: 0.13795592185369757\n",
            "Epoch 11/100, Loss: 0.12227596619794535\n",
            "Epoch 12/100, Loss: 0.11004367054894913\n",
            "Epoch 13/100, Loss: 0.1012309268116951\n",
            "Epoch 14/100, Loss: 0.0890750071510326\n",
            "Epoch 15/100, Loss: 0.07977515606339587\n",
            "Epoch 16/100, Loss: 0.0748008579708809\n",
            "Epoch 17/100, Loss: 0.06452052979621777\n",
            "Epoch 18/100, Loss: 0.058211231907439785\n",
            "Epoch 19/100, Loss: 0.05291073084917179\n",
            "Epoch 20/100, Loss: 0.04794657390651315\n",
            "Epoch 21/100, Loss: 0.043143967253177665\n",
            "Epoch 22/100, Loss: 0.03963507144430349\n",
            "Epoch 23/100, Loss: 0.03573067053112873\n",
            "Epoch 24/100, Loss: 0.032933546186879624\n",
            "Epoch 25/100, Loss: 0.031218532039675603\n",
            "Epoch 26/100, Loss: 0.028806353507693425\n",
            "Epoch 27/100, Loss: 0.02700705471080403\n",
            "Epoch 28/100, Loss: 0.024124549463564574\n",
            "Epoch 29/100, Loss: 0.02271577432145213\n",
            "Epoch 30/100, Loss: 0.020937018313033635\n",
            "Epoch 31/100, Loss: 0.01965803676826316\n",
            "Epoch 32/100, Loss: 0.01850602095730083\n",
            "Epoch 33/100, Loss: 0.017343866920401883\n",
            "Epoch 34/100, Loss: 0.01660100058760754\n",
            "Epoch 35/100, Loss: 0.015520515375185845\n",
            "Epoch 36/100, Loss: 0.014778983746763579\n",
            "Epoch 37/100, Loss: 0.01554605668020803\n",
            "Epoch 38/100, Loss: 0.014343075280965761\n",
            "Epoch 39/100, Loss: 0.013342246066692263\n",
            "Epoch 40/100, Loss: 0.013821059967889342\n",
            "Epoch 41/100, Loss: 0.012731228218695451\n",
            "Epoch 42/100, Loss: 0.012012992140858672\n",
            "Epoch 43/100, Loss: 0.011850558362121498\n",
            "Epoch 44/100, Loss: 0.011423243740356939\n",
            "Epoch 45/100, Loss: 0.010864930563108173\n",
            "Epoch 46/100, Loss: 0.010066974832308154\n",
            "Epoch 47/100, Loss: 0.009909501808240663\n",
            "Epoch 48/100, Loss: 0.009461826688155185\n",
            "Epoch 49/100, Loss: 0.00922590356623364\n",
            "Epoch 50/100, Loss: 0.008964452621808578\n",
            "Epoch 51/100, Loss: 0.011799058324635722\n",
            "Epoch 52/100, Loss: 0.021030726257798282\n",
            "Epoch 53/100, Loss: 0.015203315331492313\n",
            "Epoch 54/100, Loss: 0.012582199019921381\n",
            "Epoch 55/100, Loss: 0.011076062018874773\n",
            "Epoch 56/100, Loss: 0.011926806607651849\n",
            "Epoch 57/100, Loss: 0.010245427196888729\n",
            "Epoch 58/100, Loss: 0.008980759820186122\n",
            "Epoch 59/100, Loss: 0.00860767927410644\n",
            "Epoch 60/100, Loss: 0.00793435424566269\n",
            "Epoch 61/100, Loss: 0.007691051453611878\n",
            "Epoch 62/100, Loss: 0.007676294625758431\n",
            "Epoch 63/100, Loss: 0.00703123074328137\n",
            "Epoch 64/100, Loss: 0.006883371177367693\n",
            "Epoch 65/100, Loss: 0.0064446612174601054\n",
            "Epoch 66/100, Loss: 0.006269347215028003\n",
            "Epoch 67/100, Loss: 0.006226527558769597\n",
            "Epoch 68/100, Loss: 0.005789327716758085\n",
            "Epoch 69/100, Loss: 0.005910392818149439\n",
            "Epoch 70/100, Loss: 0.005677786060110774\n",
            "Epoch 71/100, Loss: 0.005439236630187478\n",
            "Epoch 72/100, Loss: 0.005430669599580903\n",
            "Epoch 73/100, Loss: 0.005509685562533694\n",
            "Epoch 74/100, Loss: 0.0055283464621319325\n",
            "Epoch 75/100, Loss: 0.005228507808994415\n",
            "Epoch 76/100, Loss: 0.005298288143781382\n",
            "Epoch 77/100, Loss: 0.005108463642902152\n",
            "Epoch 78/100, Loss: 0.005073659230283526\n",
            "Epoch 79/100, Loss: 0.0049328797680953905\n",
            "Epoch 80/100, Loss: 0.005202602746701518\n",
            "Epoch 81/100, Loss: 0.005198347122343474\n",
            "Epoch 82/100, Loss: 0.005269771874990574\n",
            "Epoch 83/100, Loss: 0.005082349552837915\n",
            "Epoch 84/100, Loss: 0.004965905560353814\n",
            "Epoch 85/100, Loss: 0.004878566091403712\n",
            "Epoch 86/100, Loss: 0.004792863995833106\n",
            "Epoch 87/100, Loss: 0.004628538191968272\n",
            "Epoch 88/100, Loss: 0.004727005195089205\n",
            "Epoch 89/100, Loss: 0.004654151696148653\n",
            "Epoch 90/100, Loss: 0.004675411148170053\n",
            "Epoch 91/100, Loss: 0.0045665464591408194\n",
            "Epoch 92/100, Loss: 0.004614574027919146\n",
            "Epoch 93/100, Loss: 0.004449622553968152\n",
            "Epoch 94/100, Loss: 0.004403300973218541\n",
            "Epoch 95/100, Loss: 0.0043218206386839925\n",
            "Epoch 96/100, Loss: 0.0044013426400894345\n",
            "Epoch 97/100, Loss: 0.00421882777063306\n",
            "Epoch 98/100, Loss: 0.004271448535714732\n",
            "Epoch 99/100, Loss: 0.004494080845223263\n",
            "Epoch 100/100, Loss: 0.004361624564152471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/chatbot_models/roberta_mh.pth')"
      ],
      "metadata": {
        "id": "BLgZdW9VM5Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/chatbot_models/roberta_mh.pth', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "mqDB45uj3whX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chatbot**"
      ],
      "metadata": {
        "id": "kXO6NsJfXqb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(model, tokenizer, device):\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        # Tokenize user input\n",
        "        user_input_ids = tokenizer.encode(user_input, return_tensors='pt').to(device)\n",
        "        # Generate response\n",
        "        bot_output = model.generate(user_input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id, temperature=0.7, do_sample=True)\n",
        "        # Decode and print response\n",
        "        bot_response = tokenizer.decode(bot_output[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot:\", bot_response)\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "# Start chatting\n",
        "chat_with_bot(model, tokenizer, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLb9SsguO1CK",
        "outputId": "031d5dfb-c5c1-49fa-8991-42f25f5cc765"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: i am really stressed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: i am really stressed<ASSISTANT>: I'm really sorry to hear that you're feeling overwhelmed. Please please consider talking to a mental health professional, such as a psychiatrist, therapist, or a counselor.\n",
            "You: i am not feeling good\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: i am not feeling good<ASSISTANT>: It's okay to feel down about yourself, and I'm really sorry to hear that you're feeling this way. I can offer some suggestions that might help you feel better.\n",
            "\n",
            "1\n",
            "You: what are those suggestions?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: what are those suggestions?<ASSISTANT>: The challenges you can address to find a suitable match for your needs:\n",
            "\n",
            "1. Seek Professional Help: Seek professional help: Engage in a comprehensive assessment, such as a virtual therapy\n",
            "You: no \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: no <ASSISTANT>: Here are some suggestions:\n",
            "\n",
            "1. Consider talking to a mental health professional: Psychologist or psychiatrist who specializes in psychotherapy and is a qualified mental health professional.\n",
            "\n",
            "2. Consult a licensed\n",
            "You: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: no<HUMAN>: It's okay that people have feelings of feelings of loss and sadness. Here are some suggestions:\n",
            "\n",
            "1. Talk to a Mental Health Professional: Talk to a mental health professional, such as psychologists, psychiatrists\n",
            "You: no i am scared\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: no i am scared<ASSISTANT>: The stigma is there to stigmatize stigmatized mental health as they are considered stigmatized and stigmatized. There is stigma towards mental health professionals who don't get a job in healthcare but they must\n",
            "You: i didnt get you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: i didnt get you<ASSISTANT>: I understand that you're not ready for any of of these things. \n",
            "\n",
            "1. Sleep Disorders: The sleep disturbances can interfere with your sleep quality. If you find that some of these\n",
            "You: bye\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: bye<ASSISTANT>: Hello,, many people in India. It's okay to feel overwhelmed with your feelings. Please don't hesitate to seek help immediately. Please don't hesitate to reach out to your representatives.\n",
            "You: quit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: quit<ASSISTANT>: Why are people becoming more hopeless?\n",
            "\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation**"
      ],
      "metadata": {
        "id": "xiNbCt2aXvNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForCausalLM\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "dataset = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\")\n",
        "texts = dataset['train']['text']\n",
        "\n",
        "# Step 2: Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Step 3: Tokenize the conversations\n",
        "max_seq_length = 512  # Maximum sequence length supported by RoBERTa\n",
        "tokenized_conversations = []\n",
        "for text in texts:\n",
        "    tokenized_text = tokenizer.encode(text, max_length=max_seq_length, truncation=True)\n",
        "    tokenized_conversations.append(torch.tensor(tokenized_text))\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, conversations, max_seq_length):\n",
        "        self.conversations = conversations\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        # Pad or truncate the conversation to the maximum sequence length\n",
        "        conversation = conversation[:self.max_seq_length]\n",
        "        # Pad the conversation if it's shorter than max_seq_length\n",
        "        conversation = torch.nn.functional.pad(conversation, (0, self.max_seq_length - len(conversation)))\n",
        "        # Return both the input_ids and target_ids\n",
        "        return conversation, conversation\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ChatDataset(tokenized_conversations, max_seq_length)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = RobertaForCausalLM.from_pretrained(\"/content/drive/MyDrive/chatbot_models/roberta.pth\")\n",
        "\n",
        "# Create a data loader with custom collation function\n",
        "def collate_fn(batch):\n",
        "    input_ids, target_ids = zip(*batch)\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    return input_ids, target_ids\n",
        "\n",
        "batch_size = 8\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0\n",
        "total_accuracy = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs, labels=targets)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "        accuracy = (predicted_ids == targets).float().mean().item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "avg_loss = total_loss / len(test_loader)\n",
        "avg_accuracy = total_accuracy / len(test_loader)\n",
        "\n",
        "print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbAtUTNOODxn",
        "outputId": "cab9e9a9-1b1e-4450-bf52-8002f4b22600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.0110\n",
            "Average Accuracy: 0.5768\n"
          ]
        }
      ]
    }
  ]
}